{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0699e1a7",
   "metadata": {},
   "source": [
    "# A Machine Learning Approach to Reducing Missed Appointments to Improve health efficiency and Resource Utilisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1479cd4",
   "metadata": {},
   "source": [
    "Objective:\n",
    "To reduce missed medical appointments (no-shows) by predicting patient attendance \n",
    "using SQL-based data preprocessing, GitHub version control, Tableau for visual insight, \n",
    "and Python machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0d26aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "# Core libraries for data manipulation and operations\n",
    "import pandas as pd                      # For data loading, manipulation, and analysis\n",
    "import numpy as np                       # For numerical operations and array handling\n",
    "from collections import Counter          # For counting class distributions\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt          # For creating static, animated, and interactive plots\n",
    "import seaborn as sns                    # For enhanced data visualization (built on top of matplotlib)\n",
    "sns.set(style='whitegrid')               # Use a clean white grid style for all seaborn plots\n",
    "\n",
    "# SHAP for model explainability\n",
    "import shap                              # For interpreting model predictions (especially tree-based models)\n",
    "\n",
    "# Scikit-learn: model selection, preprocessing, and pipeline management\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,                    # To split the dataset into training and testing sets\n",
    "    GridSearchCV,                        # For hyperparameter tuning using cross-validation\n",
    "    StratifiedKFold                      # For stratified cross-validation\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler     # For scaling numeric features to standard normal\n",
    "from sklearn.pipeline import Pipeline                 # For creating streamlined ML workflows\n",
    "\n",
    "# Classification performance metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,                      # Overall accuracy of predictions\n",
    "    balanced_accuracy_score,             # Adjusted accuracy score for imbalanced datasets\n",
    "    precision_score,                     # Precision: proportion of true positives among predicted positives\n",
    "    recall_score,                        # Recall: proportion of true positives among actual positives\n",
    "    f1_score,                            # Harmonic mean of precision and recall\n",
    "    roc_auc_score,                       # Area under the ROC curve\n",
    "    roc_curve,                           # ROC curve values (TPR, FPR)\n",
    "    classification_report,              # Complete performance summary (precision, recall, f1-score)\n",
    "    confusion_matrix,                   # Matrix of actual vs predicted classes\n",
    "    ConfusionMatrixDisplay              # Visual display of confusion matrix\n",
    ")\n",
    "\n",
    "# Classification models\n",
    "from sklearn.linear_model import LogisticRegression         # Logistic Regression classifier\n",
    "from sklearn.tree import DecisionTreeClassifier             # Decision Tree classifier\n",
    "from sklearn.ensemble import RandomForestClassifier         # Random Forest classifier (ensemble of trees)\n",
    "from xgboost import XGBClassifier                           # Extreme Gradient Boosting (XGBoost) classifier\n",
    "\n",
    "# Sampling techniques for handling class imbalance\n",
    "from imblearn.over_sampling import SMOTE                    # Synthetic Minority Over-sampling Technique\n",
    "from imblearn.under_sampling import (\n",
    "    RandomUnderSampler,                                     # Randomly under-samples majority class\n",
    "    NeighbourhoodCleaningRule                               # Removes ambiguous/noisy instances from majority class\n",
    ")\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline       # Pipeline combining sampling and modeling (avoid conflict with sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859a6111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preview the dataset\n",
    "df = pd.read_csv('Patient_Appointment_Attendance.csv')\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display dataset information\n",
    "print(\"\\nDataset info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74107967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows: {duplicate_count}\")\n",
    "\n",
    "# Check for missing values in each column\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "print(\"\\nMissing values in each column (if any):\")\n",
    "print(missing_values)\n",
    "\n",
    "# Check number of unique values for each column\n",
    "unique_values = df.nunique().sort_values(ascending=False)\n",
    "print(\"\\nUnique values in each column:\")\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81dabce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine unique values in 'Handcap' column before transformation\n",
    "unique_handcap_values = df['Handcap'].value_counts().sort_index()\n",
    "print(\"\\nUnique values in 'Handcap' column before binarization:\")\n",
    "print(unique_handcap_values)\n",
    "\n",
    "# Binarize 'Handcap': 0 for no disability, 1 for any level of disability\n",
    "df['Handcap'] = df['Handcap'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Confirm the changes to 'Handcap'\n",
    "handcap_binarized_counts = df['Handcap'].value_counts()\n",
    "print(\"\\nCounts of binarized 'Handcap' values:\")\n",
    "print(handcap_binarized_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b65b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine unique values in 'Age' column before transformation\n",
    "unique_age_values = df['Age'].value_counts().sort_index()\n",
    "print(\"\\nUnique values in 'Age' column before binarization:\")\n",
    "print(unique_age_values)\n",
    "\n",
    "# Drop rows with invalid age values\n",
    "df = df[df['Age'] >= 0]\n",
    "\n",
    "# Confirm the update\n",
    "print(\"Minimum age after cleaning:\", df['Age'].min())\n",
    "print(\"Maximum age after cleaning:\", df['Age'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4697d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode 'Gender' as binary: 0 for Female, 1 for Male\n",
    "df['Gender'] = df['Gender'].map({'F': 0, 'M': 1})\n",
    "print(\"\\nUnique values in 'Gender' after encoding:\")\n",
    "print(df['Gender'].value_counts())\n",
    "\n",
    "# Convert date columns to datetime objects with timezone awareness\n",
    "df['ScheduledDay'] = pd.to_datetime(df['ScheduledDay'], utc=True)\n",
    "df['AppointmentDay'] = pd.to_datetime(df['AppointmentDay'], utc=True)\n",
    "\n",
    "# Create 'LeadTime' feature: days between scheduling and appointment\n",
    "df['LeadTime'] = (df['AppointmentDay'] - df['ScheduledDay']).dt.days\n",
    "\n",
    "# Extract day of the week for the appointment\n",
    "df['AppointmentWeekday'] = df['AppointmentDay'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "\n",
    "# Encode 'No-show' as binary: 0 = showed up, 1 = no-show\n",
    "df['No_show_Int'] = df['No-show'].map({'No': 0, 'Yes': 1})\n",
    "print(\"\\nEncoded 'No_show_Int' column value counts:\")\n",
    "print(df['No_show_Int'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b7719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine unique values in 'LeadTime' column before transformation\n",
    "unique_leadtime_values = df['LeadTime'].value_counts().sort_index()\n",
    "print(\"\\nUnique values in 'LeadTime' column before binarization:\")\n",
    "print(unique_leadtime_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e3e55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display dataset information\n",
    "print(\"\\nDataset info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39aef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows with negative LeadTime\n",
    "negative_leadtime_rows = df[df['LeadTime'] < 0]\n",
    "\n",
    "# Print the number of rows with negative LeadTime\n",
    "print(\"Number of rows with negative LeadTime:\", negative_leadtime_rows.shape[0])\n",
    "\n",
    "# Print the first 5 rows with negative LeadTime for inspection\n",
    "print(\"\\nSample rows with negative LeadTime:\")\n",
    "print(negative_leadtime_rows.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643fcd9a",
   "metadata": {},
   "source": [
    "\n",
    "It was observed that some values in the `LeadTime` column were negative. \n",
    "Further inspection revealed that in these cases, the `ScheduledDay` was recorded as occurring after the `AppointmentDay`, \n",
    "which is logically incorrect. Specifically, entries with a `LeadTime` of -1 corresponded to same-day scheduling and appointment. \n",
    "These values were therefore recoded to 0. \n",
    "Rows with `LeadTime` values less than -1 were considered erroneous and removed from the dataset, \n",
    "as they were both few in number and unlikely to contribute meaningfully to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c590768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert LeadTime == -1 to 0 (same-day scheduling and appointment)\n",
    "df.loc[df['LeadTime'] == -1, 'LeadTime'] = 0\n",
    "\n",
    "# Drop rows with LeadTime < -1 (invalid scheduling)\n",
    "df = df[df['LeadTime'] >= -1]  # this now drops only rows less than -1\n",
    "\n",
    "# Confirm the update\n",
    "print(\"Number of rows after cleaning LeadTime:\", df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5900513",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Last 5 rows of the dataset:\")\n",
    "print(df.tail())\n",
    "\n",
    "# Display dataset information\n",
    "print(\"\\nDataset info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b72418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the biased dataset\n",
    "df.to_csv(\"Preprocessed_patient_appointment.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1c421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "df.drop(columns=[\"AppointmentID\", \"PatientId\", \"ScheduledDay\", \"AppointmentDay\", \"No-show\", \"AppointmentWeekday\"], inplace=True)\n",
    "\n",
    "# Confirm remaining columns\n",
    "print(\"\\nRemaining columns after drop:\")\n",
    "print(df.columns)\n",
    "\n",
    "print(\"Last 5 rows of the dataset:\")\n",
    "print(df.tail())\n",
    "\n",
    "# Display dataset information\n",
    "print(\"\\nDataset info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de68779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns only\n",
    "numeric_df = df.select_dtypes(include='number')\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = numeric_df.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "# Add title and layout\n",
    "plt.title(\"Correlation Heatmap (Numeric Features Only)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aee51dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get value counts\n",
    "no_show_counts = df['No_show_Int'].value_counts().sort_index()  # 0: showed up, 1: no-show\n",
    "\n",
    "# Define labels\n",
    "labels = ['Showed Up', 'No-Show']\n",
    "\n",
    "# Define colors (optional)\n",
    "colors = ['#66b3ff', '#ff9999']\n",
    "\n",
    "# Plot pie chart\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(no_show_counts, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Patient Appointment Attendance Distribution')\n",
    "plt.axis('equal')  # Equal aspect ratio ensures the pie chart is a circle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a21740",
   "metadata": {},
   "source": [
    "It was observed that the dataset was imbalanced, with a significant bias toward the majority class — patients who showed up for \n",
    "their appointments. This imbalance posed a risk of skewing the machine learning models toward favouring the majority class, \n",
    "potentially reducing their ability to accurately predict no-show instances. \n",
    "To address this, bias-mitigation techniques such as resampling methods were employed \n",
    "to ensure fairer and more reliable model performance across both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ed8d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random sample of 20000 rows from the original DataFrame\n",
    "df_sample = df.sample(n=20000, random_state=42)  # random_state for reproducibility\n",
    "\n",
    "\n",
    "# Features and target\n",
    "X = df_sample.drop(columns=['No_show_Int'])\n",
    "y = df_sample['No_show_Int']\n",
    "\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145aa8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, roc_auc_score,\n",
    "    precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix, roc_curve\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler, NeighbourhoodCleaningRule\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=UndefinedMetricWarning)\n",
    "\n",
    "# Store feature names\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "# Train-test split and scale\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Sampling strategies\n",
    "sampling_strategies = {\n",
    "    'Basemodel': None,\n",
    "    'NCR': NeighbourhoodCleaningRule(),\n",
    "    'RUS': RandomUnderSampler(random_state=42),\n",
    "    'SMOTE': SMOTE(random_state=42)\n",
    "}\n",
    "\n",
    "# Models and hyperparameter grids\n",
    "model_grid = {\n",
    "    'LogisticRegression': {\n",
    "        'model': LogisticRegression(max_iter=1000),\n",
    "        'params': {'model__C': [0.01, 0.1, 1, 10]}\n",
    "    },\n",
    "    'DecisionTree': {\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'params': {'model__max_depth': [3, 5, 10]}\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {'model__n_estimators': [50, 100], 'model__max_depth': [5, 10]}\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBClassifier(eval_metric='logloss'),\n",
    "        'params': {'model__n_estimators': [50, 100], 'model__max_depth': [3, 5]}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Storage\n",
    "results = []\n",
    "roc_data = {}\n",
    "feature_importances = {}\n",
    "shap_values_store = {}\n",
    "conf_matrices = []\n",
    "conf_titles = []\n",
    "\n",
    "# SHAP config\n",
    "shap_models = ['XGBoost']\n",
    "shap_samplers = sampling_strategies.keys()\n",
    "\n",
    "# Training and evaluation\n",
    "for sampler_label, sampler in sampling_strategies.items():\n",
    "    print(f\"\\n--- Running models with {sampler_label} data ---\")\n",
    "\n",
    "    if sampler:\n",
    "        X_train_res, y_train_res = sampler.fit_resample(X_train, y_train)\n",
    "    else:\n",
    "        X_train_res, y_train_res = X_train.copy(), y_train.copy()\n",
    "\n",
    "    print(\"y_train:\", Counter(y_train_res))\n",
    "    print(\"y_test :\", Counter(y_test))\n",
    "    print(\"X_train shape:\", X_train_res.shape)\n",
    "    print(\"X_test shape :\", X_test.shape)\n",
    "\n",
    "    for model_name, config in model_grid.items():\n",
    "        try:\n",
    "            print(f\"Training {model_name} with {sampler_label} sampling...\")\n",
    "            pipeline = Pipeline([('model', config['model'])])\n",
    "            grid = GridSearchCV(pipeline, config['params'], cv=3, scoring='roc_auc', n_jobs=-1)\n",
    "            grid.fit(X_train_res, y_train_res)\n",
    "\n",
    "            y_pred = grid.predict(X_test)\n",
    "            y_proba = grid.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            # Store metrics + best params\n",
    "            best_params = grid.best_params_\n",
    "            results.append({\n",
    "                'model': f\"{model_name} ({sampler_label})\",\n",
    "                'accuracy': accuracy_score(y_test, y_pred),\n",
    "                'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),\n",
    "                'roc_auc': roc_auc_score(y_test, y_proba),\n",
    "                'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "                'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "                'f1_score': f1_score(y_test, y_pred, zero_division=0),\n",
    "                'best_params': best_params\n",
    "            })\n",
    "\n",
    "            print(classification_report(y_test, y_pred))\n",
    "            print(f\"Best Params for {model_name} ({sampler_label}): {best_params}\")\n",
    "\n",
    "            # Confusion matrix\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            conf_matrices.append(cm)\n",
    "            conf_titles.append(f\"{model_name} ({sampler_label})\")\n",
    "\n",
    "            # ROC curve\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "            roc_data[f\"{model_name} ({sampler_label})\"] = (fpr, tpr)\n",
    "\n",
    "            # Feature importance\n",
    "            base_model = grid.best_estimator_.named_steps['model']\n",
    "            if hasattr(base_model, 'feature_importances_'):\n",
    "                feature_importances[f\"{model_name} ({sampler_label})\"] = base_model.feature_importances_\n",
    "            elif hasattr(base_model, 'coef_'):\n",
    "                feature_importances[f\"{model_name} ({sampler_label})\"] = np.abs(base_model.coef_[0])\n",
    "            else:\n",
    "                print(f\"No feature importance for {model_name} ({sampler_label})\")\n",
    "\n",
    "            # SHAP\n",
    "            if model_name in shap_models and sampler_label in shap_samplers:\n",
    "                print(f\"Generating SHAP values for {model_name} ({sampler_label})...\")\n",
    "                explainer = shap.Explainer(base_model, X_train_res)\n",
    "                shap_values = explainer(X_test)\n",
    "                shap_values_store[f\"{model_name} ({sampler_label})\"] = shap_values\n",
    "\n",
    "                shap.summary_plot(shap_values, X_test, feature_names=feature_names, show=False)\n",
    "                plt.title(f\"SHAP Summary - {model_name} ({sampler_label})\")\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error training {model_name} ({sampler_label}): {e}\")\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(results_df[['model', 'accuracy', 'balanced_accuracy', 'roc_auc', 'best_params']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14f846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ROC Curves with AUC Values in Legend ---\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Build a lookup for AUC values\n",
    "auc_lookup = dict(zip(results_df['model'], results_df['roc_auc']))\n",
    "\n",
    "# Plot each ROC curve\n",
    "for label, (fpr, tpr) in roc_data.items():\n",
    "    auc = auc_lookup.get(label, None)\n",
    "    label_with_auc = f\"{label} (AUC = {auc:.3f})\" if auc is not None else label\n",
    "    plt.plot(fpr, tpr, label=label_with_auc)\n",
    "\n",
    "# Plot baseline\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "plt.title('ROC Curves for All Models and Sampling Techniques')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right', fontsize='small')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Feature Importance Plots for All Models and Sampling Techniques ---\n",
    "for model_label, importances in feature_importances.items():\n",
    "    try:\n",
    "        # Convert to Series for better plotting\n",
    "        imp_series = pd.Series(importances, index=feature_names)\n",
    "        imp_series = imp_series.sort_values(ascending=False)[:10]  # Top 10 features\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=imp_series.values, y=imp_series.index)\n",
    "        plt.title(f\"Top 10 Feature Importances - {model_label}\")\n",
    "        plt.xlabel(\"Importance Score\")\n",
    "        plt.ylabel(\"Feature\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting feature importance for {model_label}: {e}\")\n",
    "\n",
    "\n",
    "# --- Accuracy and Balanced Accuracy Bar Charts ---\n",
    "results_df_sorted = results_df.sort_values(by='model')\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Accuracy\n",
    "axes[0].bar(results_df_sorted['model'], results_df_sorted['accuracy'])\n",
    "axes[0].set_title('Accuracy by Model and Sampling Strategy')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].tick_params(axis='x', rotation=90)\n",
    "for idx, value in enumerate(results_df_sorted['accuracy']):\n",
    "    axes[0].text(idx, value + 0.01, f\"{value:.1%}\", ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Balanced Accuracy\n",
    "axes[1].bar(results_df_sorted['model'], results_df_sorted['balanced_accuracy'])\n",
    "axes[1].set_title('Balanced Accuracy by Model and Sampling Strategy')\n",
    "axes[1].set_ylabel('Balanced Accuracy')\n",
    "axes[1].tick_params(axis='x', rotation=90)\n",
    "for idx, value in enumerate(results_df_sorted['balanced_accuracy']):\n",
    "    axes[1].text(idx, value + 0.01, f\"{value:.1%}\", ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Confusion Matrix Grid (4x4) ---\n",
    "fig, axes = plt.subplots(4, 4, figsize=(18, 18))\n",
    "fig.suptitle(\"Confusion Matrices for All Models and Sampling Strategies\", fontsize=18)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < len(conf_matrices):\n",
    "        sns.heatmap(conf_matrices[i], annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                    xticklabels=['No Show = 0', 'No Show = 1'],\n",
    "                    yticklabels=['No Show = 0', 'No Show = 1'], ax=ax)\n",
    "        ax.set_title(conf_titles[i], fontsize=10)\n",
    "        ax.set_xlabel(\"Predicted\")\n",
    "        ax.set_ylabel(\"Actual\")\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decd1785",
   "metadata": {},
   "source": [
    "Accuracy, Balanced Accuracy, and AUC-ROC were selected as the key evaluation metrics because they provide a balanced and interpretable view of model performance, particularly in imbalanced classification tasks like healthcare predictions. Accuracy reflects the proportion of total correct predictions, including both true positives and true negatives. However, when one class dominates the dataset, accuracy alone can be misleading. Balanced Accuracy addresses this by averaging sensitivity (the model’s ability to correctly identify true positives) and specificity (its ability to correctly identify true negatives), offering a fairer reflection of performance across both classes (Guesné et al., 2024). AUC-ROC complements these by measuring how well the model separates positive from negative cases across all thresholds, providing a threshold-independent indicator of classification quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651ff0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Percentage Difference Compared to Basemodel (Core Metrics Only) ===\n",
    "# Identify key metrics\n",
    "results_df_renamed = results_df.rename(columns={\n",
    "    'accuracy': 'Accuracy',\n",
    "    'balanced_accuracy': 'Balanced Accuracy',\n",
    "    'roc_auc': 'ROC-AUC Score'\n",
    "})\n",
    "\n",
    "# Extract model name and sampling strategy\n",
    "results_df_renamed['Model'] = results_df_renamed['model'].apply(lambda x: x.split(' ')[0])\n",
    "results_df_renamed['Sampling'] = results_df_renamed['model'].apply(lambda x: x.split('(')[-1].replace(')', ''))\n",
    "\n",
    "# Select only core metrics\n",
    "core_metrics = ['Accuracy', 'Balanced Accuracy', 'ROC-AUC Score']\n",
    "\n",
    "# Split into baseline and comparison sets\n",
    "base_df = results_df_renamed[results_df_renamed['Sampling'] == 'Basemodel'].set_index('Model')\n",
    "comparison_df = results_df_renamed[results_df_renamed['Sampling'] != 'Basemodel'].set_index('Model')\n",
    "\n",
    "# Join comparison with baseline metrics\n",
    "comparison_with_base = comparison_df.join(\n",
    "    base_df[core_metrics],\n",
    "    lsuffix='_Current',\n",
    "    rsuffix='_Basemodel',\n",
    "    on='Model'\n",
    ").reset_index()\n",
    "\n",
    "# Compute % differences\n",
    "for metric in core_metrics:\n",
    "    current = f\"{metric}_Current\"\n",
    "    base = f\"{metric}_Basemodel\"\n",
    "    comparison_with_base[f\"{metric} % Diff\"] = (\n",
    "        (comparison_with_base[current] - comparison_with_base[base]) / comparison_with_base[base]\n",
    "    ) * 100\n",
    "\n",
    "# Create summary table\n",
    "percent_diff_cols = ['Model', 'Sampling'] + [f\"{metric} % Diff\" for metric in core_metrics]\n",
    "percent_diff_df = comparison_with_base[percent_diff_cols]\n",
    "\n",
    "# Display result\n",
    "print(\"\\n=== Percentage Difference Compared to Basemodel (Core Metrics Only) ===\")\n",
    "print(percent_diff_df.to_string(index=False))\n",
    "\n",
    "# Prepare for heatmap\n",
    "melted_pct = pd.melt(\n",
    "    percent_diff_df,\n",
    "    id_vars=['Model', 'Sampling'],\n",
    "    var_name='Metric',\n",
    "    value_name='Percentage Difference'\n",
    ")\n",
    "\n",
    "pivot_pct = melted_pct.pivot_table(index=['Model', 'Sampling'], columns='Metric', values='Percentage Difference')\n",
    "\n",
    "# --- Clean up for heatmap ---\n",
    "pivot_pct.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "pivot_pct.fillna(0, inplace=True)\n",
    "pivot_pct = pivot_pct.clip(lower=-100, upper=100)\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 2 + 0.5 * len(pivot_pct)))\n",
    "sns.heatmap(pivot_pct, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0, linewidths=0.5)\n",
    "\n",
    "# Title\n",
    "strategies = results_df_renamed['Sampling'].unique().tolist()\n",
    "if 'Basemodel' in strategies:\n",
    "    strategies.remove('Basemodel')\n",
    "strategy_title = \" vs \".join(strategies)\n",
    "plt.title(f\"Percentage Difference (Accuracy, Balanced Accuracy, ROC-AUC) vs Basemodel ({strategy_title})\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddb8df3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
